{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Multiple time series analysis for Main-Post employee outages","text":"<p>Postal carriers are the backbone of last-mile logistics. However, due to the increased volume of deliveries, changing weather conditions and the often high workload, postal carriers are significantly affected by sick days and absences. In collaboration with the Main-Post media group, we have developed a data-driven tool for analysing and predicting absences.  Our application makes it possible to predict the cancellations of the Main-Post delivery staff. To do this, historical cancellations as well as historical and future weather data are taken into account. The optimisation framework can also be used to determine the best possible replacement for missing employees.  The following visualisation gives an overview of the functionalities of our application:  The objective of this project documentation is to provide an overview of our application as well as a deeper insight into the structure of the application. This documentation explains the tools and technologies used and how we technically implemented the individual components.   Our project documentation consists of the following components: </p> Project Setup Architecture Data Overview Services Web Application Backend"},{"location":"architecture/auth/","title":"Authentification with Supabase","text":"Authentication within Supabase plays a significant role in ensuring security and privacy in applications. It provides developers with the necessary resources to securely manage user accounts and effectively regulate access rights, while providing a user-friendly login procedure for end users.      <p>important features and details of the Supabase authentification:</p> <ul> <li>User management: Supabase provides user management tools that allow developers to create, edit and manage user accounts. This includes the ability to create new users, update user profiles and deactivate or delete user accounts. </li> <li>Authentication methods: Supabase supports various authentication methods, including email/password, social logins (e.g. via Google, GitHub, or other identity providers), and authentication via JSON Web Tokens (JWT). This allows users to log in in the way that is most convenient for them. </li> <li>Permissions: Supabase allows the definition of permission rules for users. Developers can control exactly which data and functions users are allowed to access, based on roles and rules defined in the application. This contributes to the security and privacy of the application. </li> <li>SSO (Single Sign-On): Supabase also enables the implementation of Single Sign-On (SSO) for applications. With SSO, users can log in to different applications via a single user account, which increases the user experience. </li> <li>Password reset: Supabase offers password reset functions that allow users to reset their password if they have forgotten it. This increases usability and reduces the effort for the support team. </li> <li>Real-time authentication: Authentication in Supabase is real-time, which means that users have instant access to their accounts and permissions without having to log in again.  </li> </ul>"},{"location":"architecture/backend/","title":"Backend","text":""},{"location":"architecture/backend/#fastapi","title":"FastAPI","text":"FastAPI is a modern Python framework for developing web applications and APIs.      <p>Key features of FastAPI:</p> <ul> <li>Speed: FastAPI is very fast due to its use of asynchronous Python features and optimised techniques. </li> <li>Typing: FastAPI uses Python types to define your API endpoints. This facilitates static analysis and helps create secure APIs. </li> <li>Simple syntax: FastAPI uses Python code annotations to define routes and data models. This makes the code readable and understandable. </li> <li>Support for WebSockets: FastAPI offers support for WebSockets, so you can build real-time applications. </li> <li>Embedded validation and serialisation: FastAPI facilitates input validation and output serialisation, which simplifies the development of robust APIs. </li> </ul>"},{"location":"architecture/backend/#darts","title":"Darts","text":"Darts is a time series forecasting library in Python that focuses on ease of use, modularity, and extensibility. It offers a range of models and tools designed to simplify the process of building and deploying accurate time series forecasting models for single and multiple timeseries.      <p>Key features of Darts:</p> <ul> <li>Model Modularity: Darts provides a modular framework where various models, such as ARIMA, Exponential Smoothing, and machine learning-based models, can be easily implemented and combined to suit specific forecasting needs. </li> <li>Extensive Evaluation: The library includes extensive evaluation metrics and visualization tools. Darts allows to create probabilistic forecasts for instance to obtain confidence intervals. </li> <li>State-of-the-art Models: It incorporates state-of-the-art models like Temporal Fusion Transformers (TFTs), enabling users to leverage cutting-edge techniques for accurate predictions. </li> <li>Scalability: Darts is designed to handle large-scale time series datasets efficiently. Also, all Darts machine learning models are supported to be trained on multiple time series. </li> </ul>"},{"location":"architecture/backend/#optuna","title":"Optuna","text":"Optuna is an automated hyperparameter optimization framework that streamlines the process of tuning machine learning model parameters. It provides a flexible and efficient platform for finding the best set of hyperparameters to optimize model performance.      <p>Key features of Optuna:</p> <ul> <li>Lightweight and Versatile Architecture: Optuna has a lightweight and versatile architecture, making it easy to handle diverse tasks with minimal installation requirements.  </li> <li>Hyperparameter Search Algorithms: Optuna implements various search algorithms, including random search, Bayesian optimization and Grid search, enabling users to efficiently explore the hyperparameter space. </li> <li>Scalability and Parallelism: It offers scalability by supporting parallel execution of multiple trials, making it suitable for optimizing computationally expensive models and large hyperparameter spaces. </li> <li>Visualization and Analysis: The framework provides visualizations and analysis tools to understand the optimization process, allowing users to gain insights into parameter interactions and convergence behavior. </li> </ul>"},{"location":"architecture/backend/#mlflow","title":"MLflow","text":"MLflow is an open source platform for managing the entire machine learning lifecycle. It is designed to simplify and standardise the development, training, management and deployment of machine learning models.      <p>Key features of MLflow:</p> <ul> <li>Tracking: MLflow allows logging of experiments and tracking of metrics, parameters and models during the training process. This helps in tracking and comparing experiments to find out which model performs best. </li> <li>Projects: With MLflow you can organise machine learning projects in a consistent structure that contains all the necessary dependencies and configurations. This facilitates the reproducibility of models. </li> <li>Model Registry: The Model Registry component of MLflow allows you to manage and version models. You can organise models in a catalogue-like system to ensure that you always have access to the correct model version. </li> <li>Model Deployment: MLflow provides interfaces for deploying machine learning models in different environments, including cloud services, container orchestrations and local servers. This facilitates the transition of models from development to production. </li> <li>Support for various ML frameworks: MLflow is framework-agnostic and supports various machine learning libraries such as TensorFlow, PyTorch, scikit-learn and more. </li> <li>Integrations: There are integrations of MLflow with various tools and platforms, including Jupyter Notebooks, Apache Spark and many popular cloud platforms. </li> </ul>"},{"location":"architecture/database/","title":"Database","text":"Supabase is an open source platform designed to help developers create applications that require backend services and a database. This platform provides a variety of tools and services that enable developers to build web applications and mobile apps more efficiently without having to worry about the complexities of managing and scaling the backend infrastructure.      <p>We decided to use the web-based database technology Supabase because it allows us to access the database directly via the front end using the Supabase API. It also offers auxiliary functions for authentication and server-side pagination.</p> <p>important features and details of the Supabase database:</p> <ul> <li>Postgres database: Supabase is based on PostgreSQL, a powerful open source database. PostgreSQL is known for its reliability, extensibility and scalability. Supabase provides a fully managed PostgreSQL instance, allowing developers to take advantage of this database without the need for time-consuming maintenance tasks. </li> <li>ACID compliance: The Supabase database guarantees ACID properties, which stands for Atomicity, Consistency, Isolation and Durability. These properties ensure that transactions in the database are reliable and fault-tolerant. </li> <li>Scalability: The Supabase database can be scaled as needed to meet database performance and capacity requirements. This allows developers to focus on developing their applications without worrying about scaling the database. </li> <li>RESTful API: Supabase provides a RESTful API that allows developers to access the database. This allows applications to retrieve, add, update and delete data by sending HTTP requests to the API. </li> <li>Authentication and permissions: The Supabase database integrates seamlessly with the platform's authentication services. Developers can set fine-grained permissions to control access to data in the database. This facilitates the implementation of secure access rules. </li> <li>Real-time support: The database supports Supabase's real-time function, which is based on WebSockets. This makes it possible to send data changes to clients in real time, which is particularly useful when several users are working on the same data at the same time. </li> <li>Full-text search and indexing: Full-text search and indexing: The database supports full-text search and indexing, which facilitates the efficient search and retrieval of data in the database. </li> </ul> <p>The typical structure of a database query in Supabase for PostgreSQL resembles the following:</p> Database Query <pre><code>export async function fetchFilteredEmployees(startNumber, endNumber, filterOne = null, filterTwo = null) {\n  try {\n    //query to get all employees between two indizes\n    let query = supabase.from('employees').select('*').range(startNumber, endNumber);\n\n    // if filterOne and or filterTwo is set add filter to query\n    if (filterOne !== null) {\n      query = query.filter(filterOne.field, filterOne.operator, filterOne.value);\n    }\n    if (filterTwo !== null) {\n      query = query.filter(filterTwo.field, filterTwo.operator, filterTwo.value);\n    }\n\n    // perform query\n    const { data, error } = await query;\n    // console.log(data);\n\n    if (error) {\n      console.error('Fehler beim Abrufen der Daten:', error);\n      return [];\n    }\n\n    // return statement\n    return data;\n  } catch (error) {\n    console.error('Fehler beim Abrufen der Daten:', error);\n    return [];\n  }\n}\n\n</code></pre>"},{"location":"architecture/dockerizedArchitecture/","title":"Dockerized Application","text":"<p>Since the complexities in delivering our project, encompassing frontend, backend, model tracking, and a database, we have opted to containerize all components of our application using Docker containerization technology. Our project comprises a total of 4 Docker containers, all deployable with the docker-compose up command.  The default bindings are set to port 8080 for the frontend and 8000 for the backend, with the flexibility to adjust these fundamental settings using Docker-Compose and Dockerfile.   Docker was particularly suitable for our project as it enables the individual application components to be isolated in special containers. This not only ensures portability across different environments, but also improves the reproducibility of our development setups. In addition, Docker streamlines scalability and deployment through container orchestration, enabling efficient resource utilisation and providing a comprehensive solution for the development, deployment and maintenance of our interconnected project.  </p> <p> find our docker files on Github</p>"},{"location":"architecture/dockerizedArchitecture/#dockerized-vuejs-application","title":"Dockerized Vue.js Application","text":"<ul> <li>Isolation: The Vue.js application and all its dependencies, including the web server and other required software components, are isolated in a Docker container. This means that the application should run consistently regardless of the environment in which it is executed. </li> <li>Portability: A Docker container is portable and can be easily moved between different environments. This facilitates the deployment and scaling of the application in different environments, e.g. development, test and production. </li> <li>Reproducibility: With Docker, you can ensure that all developers in your team and in other environments use the same environment, which improves the reproducibility of development and test environments. </li> <li>Scalability: Docker containers can be managed in container orchestration platforms to improve application scalability and manageability.</li> </ul>"},{"location":"architecture/dockerizedArchitecture/#dockerized-backend-and-models","title":"Dockerized Backend and Models","text":"<ul> <li>Scalability and Resource Management: Separating the backend and models into distinct containers enables more efficient resource management. Additional model containers can be easily added to scale independently of the backend.</li> <li>Technological Independence: Containers allow independent development, testing, and deployment of backend and models, fostering a modular architecture. This flexibility allows choosing different technologies for each component.</li> <li>Versioning and Updates: Isolating backend and models facilitates independent versioning and updates. For example, updating a model in a container can be done without impacting the backend, easing the testing of new models and gradual application updates.</li> <li>Load Distribution: Optimizing load distribution for applications with high model traffic is achievable by using separate containers for models. This enables scaling and distributing resources based on model demand without affecting the backend.</li> <li>Independent Development Teams: Containerization supports parallel development and deployment for different teams responsible for the backend and models. As long as defined interfaces (APIs) are maintained, teams can work independently.</li> <li>Flexibility in Technology Choice: Backend and models can employ different technologies, programming languages, and frameworks. Containerization allows selecting the most suitable technologies for each element without concerns about potential conflicts.</li> </ul>"},{"location":"architecture/frontend/","title":"Frontend","text":""},{"location":"architecture/frontend/#vuejs-application","title":"Vue.js Application","text":"Vue.js is an open-source JavaScript framework mainly used for developing user interfaces of web applications and single-page applications (SPAs).      <p>After consultation with the project partner, we decided in favour of vue.js as the frontend/Javascript framework, as they also use Vue in the company. </p> <p>important features and details of Vue.js Framework: </p> <ul> <li>Lightweight: Vue.js is relatively lightweight and easy to integrate into existing projects, even if you already use other JavaScript libraries or frameworks. </li> <li>Component-based architecture: Vue.js makes it possible to split user interfaces into reusable components. </li> <li>Reactive data binding: Vue.js provides reactive data binding that allows developers to link data models and user interface components. When data changes, the user interface is automatically updated to reflect these changes. </li> <li>Directives: Vue.js has special directives that can be integrated into the HTML markup to control the application of the framework. For example, the <code>v-for</code> directive for rendering lists and the <code>v-if</code> directive for conditional rendering of elements. </li> <li>Routing: Vue Router is an official extension to Vue.js that simplifies the creation of single-page applications with client-side routing functionality. </li> <li>Tools and ecosystem: Vue.js has an active and growing community. There are numerous extensions, plugins, and tools that facilitate the development and debugging of Vue.js applications.  </li> </ul> The following image shows the dashboard of our application we created with Vue.js. <p></p>"},{"location":"architecture/overview/","title":"Overview","text":""},{"location":"backend/data_prep/","title":"Data Preparation","text":"<p>Detailed data pre-processing is essential to transform raw data into a structured, cleansed form. This improves the quality of the analysis results and enables precise predictions, which in turn strengthens decision-making and operational efficiency. The data we worked with was made available in csv files.    A central directory of our application contains essential pre-processing files as well as the implemented connectors for the Supabase database and the weather API. These files play a crucial role in the processing and integration of the data provided. These files can be found in the data_scripts subfolder of the src directory.</p>"},{"location":"backend/data_prep/#database-connector","title":"Database Connector","text":"<p>This script provides functions for interacting with our Supabase database. The functions enable data to be loaded as a DataFrame, the latest entry to be retrieved from a table, data to be inserted into a table and data to be updated in a table.</p> supabase_connector.py <pre><code>\"\"\"This script contains functions to connect to supabase. \n\nThey contain functionality for loading from supabase, inserting into supabase and updating data in supabase\n\"\"\"\n\nimport os \nfrom dotenv import load_dotenv\nfrom supabase import create_client, Client\nimport pandas as pd\nimport json \n#import retry\n\nload_dotenv()\nurl: str = os.environ.get(\"SUPABASE_URL\")\nkey: str = os.environ.get(\"SUPABASE_KEY\")\nsupabase: Client = create_client(url, key)\n\n#@retry(delay=5, tries=6)\ndef load_dataframe(name: str) -&gt; pd.DataFrame:\n    \"\"\"Load table from supabase by name and transform to pandas dataframe\"\"\"\n    data = supabase.table(name).select(\"*\").execute()\n    data_json = data.model_dump_json()\n    data_json = json.loads(data_json)\n    df = pd.DataFrame(data_json[\"data\"])\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'])\n    return df\n\ndef load_dataframe_latest(name: str) -&gt; pd.DataFrame:\n    \"\"\"Load the latest entry of a supabase table to cut waiting time\"\"\"\n    data, count = supabase.table(name).select('*').order('date', desc=True).limit(1).execute()\n    df = pd.DataFrame(data[1])\n    if 'date' in df.columns:\n        df['date'] = pd.to_datetime(df['date'])\n    return df\n\ndef insert_supabase(name: str, df: pd.DataFrame) -&gt; None:\n    \"\"\"inserts the dataframe into the supabase table\"\"\"\n    df_json = df.to_json(orient='records')\n    df_json = json.loads(df_json)\n    data = supabase.table(name).insert(df_json).execute()\n\n\ndef update_supabase(name: str, df: pd.DataFrame) -&gt; None:\n    \"\"\"Update the supabase table with the Dataframe.\"\"\"\n    data = supabase.table(\"countries\").update({\"country\": \"Indonesia\", \"capital_city\": \"Jakarta\"}).eq(\"id\", 1).execute()\n\n</code></pre>"},{"location":"backend/data_prep/#weather-connector","title":"Weather Connector","text":"<p>This script retrieves weather data from the Visual Crossing Weather API and integrates it into a DataFrame that is used to forecast absences. The functions _load_dataframe loads weather data based on a start and end date, _get_transform_weather transforms the data by encoding weather conditions one-hot, and _get_non_intersect_columns adds non-overlapping columns required for the model. Finally, the get_weather_data function returns the transformed covariates data frame, which contains weather information as well as area and reason for absence.</p> crossingweather_connector.py <pre><code>\"\"\"This file contains functions to load weather data from Visual Crossing Weather API and transform it.\n\nThere is the option to load 15 day forecast data or history data for a given daterange.\nThe data is transformed afterwards in different steps:\n    1. one-hot encoding of weather conditions\n    2. join non intersecting columns of weather and covariates dataframe\n    3. add area_id and reason to the covariates dataframe\n\"\"\"\n\nimport os\nimport pandas as pd\nimport csv\nimport codecs\n\nfrom dotenv import load_dotenv\nimport urllib.request\nimport urllib.error\nimport sys\nimport data_scripts.supabase_connector as supabase_connector\n\nload_dotenv()\nBaseURL: str = os.environ.get(\"VISUALCROSSING_URL\")\nApiKey: str = os.environ.get(\"VISUALCROSSING_KEY\")\n\nQueryLocation = '&amp;location=' + 'Wuerzburg,Germany'\nQueryKey = '&amp;key=' + ApiKey\n\ndef _load_dataframe(StartDate: str = '', EndDate: str = ''):\n    \"\"\"Load weather data from Visual Crossing Weather API.\n\n    If no start and end date is given, loads 15 day forecast data.\n    Returns the weather data as Dataframe.\n    \"\"\"\n\n    Params: str = '&amp;aggregateHours=24&amp;unitGroup=metric'#&amp;contentType=json'\n\n    if len(StartDate)&gt;0:\n        print(' - Fetching history for date: ', StartDate,'-',EndDate)\n\n        # History requests require a date.  We use the same date for start and end since we only want to query a single date in this example\n        QueryDate: str = '&amp;startDateTime=' + StartDate + 'T00:00:00&amp;endDateTime=' +EndDate + 'T00:00:00'\n        QueryTypeParams: str = 'history?'+ Params + '&amp;dayStartTime=0:0:00&amp;dayEndTime=0:0:00' + QueryDate\n\n    else:\n        print(' - Fetching forecast data')\n        QueryTypeParams: str = 'forecast?'+ Params + '&amp;shortColumnNames=false'\n\n    # Build the entire query\n    URL = BaseURL + QueryTypeParams + QueryLocation + QueryKey\n\n    try: \n        request = urllib.request.urlopen(URL)\n    except urllib.error.HTTPError  as e:\n        ErrorInfo= e.read().decode() \n        print('Error code: ', e.code, ErrorInfo)\n        sys.exit()\n    except  urllib.error.URLError as e:\n        ErrorInfo= e.read().decode() \n        print('Error code: ', e.code,ErrorInfo)\n        sys.exit()\n\n    CSVText = csv.reader(codecs.iterdecode(request, 'utf-8'))\n\n    value_list = []\n    for row in CSVText:\n        value_list.append(row)\n\n    df = pd.DataFrame(columns=value_list[0])\n    for i in range(1, len(value_list)):\n        df.loc[len(df)] = value_list[i]\n\n    return df\n\ndef _get_transform_weather(startdate: str, enddate: str) -&gt; pd.DataFrame:\n    \"\"\"Load and Transform weather data.\n\n    Call _load_dataframe and load weather data.\n    Transform weather data by one-hot encoding weather conditions and keeping necessary columns.\n    \"\"\"\n    if startdate &gt; enddate:\n        past_df = _load_dataframe()\n    else:\n        past_df = _load_dataframe(StartDate=startdate, EndDate=enddate)\n\n    past_df = past_df.rename(columns={'Date time':'date', 'Temperature': 'temp', 'Maximum Temperature':'tempmax', 'Minimum Temperature':'tempmin', 'Conditions':'conditions'})\n    past_df = past_df[['date', 'temp', 'tempmax', 'tempmin', 'conditions']]\n    past_df = pd.get_dummies(past_df, columns = ['conditions'], prefix='', prefix_sep='', dtype=int)\n    return past_df\n\ndef _get_non_intersect_columns(weather_df: pd.DataFrame, covariates_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Get the non intersecting columns of weather and covariates dataframe to create a complete Table.\n\n    The columns from the initial covariates table cover all the weather conditions and thus are used to extent the current covariates table that usually only contains a few conditions (e.g. rain, snow, fog).\n    The non intersecting column are joined to the weather dataframe and are filled with 0s. \n    \"\"\"\n    intersect_cols = set(weather_df.columns).intersection(set(covariates_df.columns))\n    non_intersect_cols = [col for col in covariates_df.columns if col not in intersect_cols]\n    non_intersect_cols.remove('id')\n    covariates_return = pd.concat([weather_df, pd.DataFrame(columns=non_intersect_cols)], axis=1)\n    covariates_return.fillna(0, inplace=True)\n    covariates_return['date'] = pd.to_datetime(covariates_return['date'])\n    return covariates_return\n\ndef _add_reason_id(noint:pd.DataFrame, covariates: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Add the area_id and reason to the covariates Dataframe.\"\"\"\n\n    noint_reason = pd.DataFrame()\n\n    for reason in covariates.reason.unique():\n        noint.reason = reason\n        noint_reason = pd.concat([noint, noint_reason])\n\n    noint_reason_area = pd.DataFrame()\n\n    for area_id in covariates.area_id.unique():\n        noint_reason.area_id = area_id\n        noint_reason_area = pd.concat([noint_reason, noint_reason_area])\n\n    noint_reason_area.reset_index(inplace=True, drop=True)\n\n    noint_reason_area['id'] = noint_reason_area.index + max(covariates['id'].astype(int))+1\n    noint_reason_area['date'] = noint_reason_area['date'].astype(str)   \n\n    return noint_reason_area\n\ndef get_weather_data(\n        covariates: pd.DataFrame, \n        StartDate: str = '', \n        EndDate: str = '') -&gt; pd.DataFrame:\n    \"\"\"Calls above functions to load and transform the weather data.\"\"\"\n    weather_df = _get_transform_weather(StartDate, EndDate)\n    no_int = _get_non_intersect_columns(weather_df, covariates)\n    noint_reason_area =_add_reason_id(no_int, covariates)\n    return noint_reason_area\n\n</code></pre>"},{"location":"backend/data_prep/#preprocess-absences","title":"Preprocess Absences","text":"<p>This script processes and transforms absence data by assigning it to an area (district) and converting it to daily data. The absence reasons 'holiday' and 'illness' are taken into account. The resulting DataFrame is then totalled and filled with zeros for days without absences. The script also excludes specific area IDs ('2.6 Z&amp;S' and '2.8 Z&amp;S') and saves the transformed DataFrame as a CSV file.</p> preprocess_absences.py <pre><code>\"\"\"This script loads and transforms the absences data.\n\nThis is a multistep process:\n    1. Transpose to daily data per row instead of ranges in columns.\n    2. Join area_id and reason to the absences\n    3. Sum up absences per day, area_id and reason\n    4. Fill Dataframe with 0 for days where there are no absences\n\nThe resulting data is used to train the timeseries forecasting model and contains the prediction variable.\nThe file is only needed for initalization of the supabase table for absences.\n\"\"\"\n\nimport pandas as pd\nimport data_scripts.supabase_connector as supabase_connector\n\ndef _join_bezirk_to_absences(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Join area_id to the absences\"\"\"\n    allocations_df = supabase_connector.load_dataframe(\"allocations\")\n    allocations_df.rename(columns={'id':'allocation_id'}, inplace=True)\n    rounds_df = supabase_connector.load_dataframe(\"districts\")\n\n    bezirk_df = allocations_df.merge(rounds_df, how='left', left_on='district_id', right_on='id')\n    bezirk_df = bezirk_df[['district_id', 'allocation_id', 'area_id']].drop_duplicates()\n\n    df = df.merge(bezirk_df, how='left', left_on='allocation_id', right_on='allocation_id')\n    return df\n\ndef load_and_preprocess_absences() -&gt; pd.DataFrame:\n    \"\"\"Load absences data and transpose to daily data instead of ranges.\n\n    Transposition is done with a list comprehension that is significantly faster then other tested methods (for loops f.e.).\n    Call _join_bezirk_to_absences to join the area_id to the absences.\n    \"\"\"\n    absences_df = supabase_connector.load_dataframe(\"absences\")\n    absences_df = absences_df.dropna(subset=['start_date', 'end_date'])\n    absences_df = absences_df[absences_df['reason'].isin(['vacation', 'illness'])]\n    absences_df = pd.concat([pd.DataFrame({'date': pd.date_range(row.start_date, row.end_date),\n               'employee_id': row.employee_id,\n               'allocation_id': row.allocation_id,\n               'id':row.id,\n               'reason':row.reason}, columns=['date', 'employee_id', 'allocation_id','id', 'reason']) \n           for i, row in absences_df.iterrows()], ignore_index=True)\n    absences_df['date'] = pd.to_datetime(absences_df['date'])\n\n    absences_df = _join_bezirk_to_absences(absences_df)\n\n    return absences_df\n\ndef _sum_df(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Sum up absences per day, area_id and reason.\"\"\"\n    df = df.drop(columns=['employee_id', 'allocation_id', 'id', 'district_id'])\n    df = df.groupby(['area_id','date', 'reason']).sum().sort_values('date').reset_index()\n    return df\n\ndef _fill_data(DataFrame: pd.DataFrame) -&gt; pd.DataFrame:  \n    \"\"\"Fill absence Dataframe with 0 for days where there are no absences.\n\n    This is important for the timeseries forecasting models with darts, because the every timeseries of the multiple timeseries need to have the same length.\n    \"\"\"\n    DataFrame_filled = DataFrame.copy()\n    for area_id in DataFrame['area_id'].unique():\n        for reason in DataFrame['reason'].unique(): \n            DataFrame_Slice = DataFrame[DataFrame['area_id']==area_id]\n            DataFrame_Slice = DataFrame_Slice[DataFrame_Slice['reason']==reason]\n            Bool_Series = pd.DataFrame({'date':DataFrame['date'].unique()})\n            Bool_Series['isinbool'] = pd.DataFrame(DataFrame['date'].unique()).isin(DataFrame_Slice['date'].unique())\n            Bool_Series = Bool_Series[Bool_Series['isinbool']==False]\n            Bool_Series['count'] = 0\n            Bool_Series['area_id'] = area_id\n            Bool_Series['reason'] = reason\n            Bool_Series=Bool_Series.drop(['isinbool'], axis=1)\n            DataFrame_filled = pd.concat([DataFrame_filled, Bool_Series])\n\n    DataFrame_filled = DataFrame_filled.reset_index(drop=True)\n    return DataFrame_filled\n\ndef transform_absences(absences_df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Transform and save absences data.\n\n    This function calls above functions and thus functions as a datapipeline with minimal transformations added.\n    That is the exclution of the area_id 2.6 Z&amp;S and 2.8 Z&amp;S. \n    \"\"\"\n    absences_df = load_and_preprocess_absences()\n    absences_df = absences_df[absences_df.area_id != '2.6 Z&amp;S']\n    absences_df = absences_df[absences_df.area_id != '2.8 Z&amp;S']\n    absences_df = absences_df.reset_index(drop=True)   \n    absences_df['count'] = 1\n\n    absences_complete = _sum_df(absences_df)\n    absences_complete = _fill_data(absences_complete)\n\n    absences_complete.to_csv('../data/processed/absences_daily_multiple_m_reason.csv', index=False)\n\n\n</code></pre>"},{"location":"backend/data_prep/#build-covariates","title":"Build Covariates","text":"<p>This script processes time series data to forecast absences. The function create_datetime_features adds temporal features to a DataFrame. The _transform_save_weather and _join_id_reason_to_covariates functions transform and integrate weather data as well as information on the area and reason for absence. The load_and_preprocess_weather function loads weather data, transforms it and adds additional information to create a standardised DataFrame for further analysis.</p> build_covariates.py <pre><code>\"\"\"This script is used to build the covariates for training the timeseries forecasting model.\n\nIt contains code that transforms the external weather data and joins it to the area and reason specific internal data.\nIs only needed for inital transformation of the external weatherdata, to initialize the supabase table for covariates.\n\"\"\"\n\nimport pandas as pd\nimport data_scripts.supabase_connector as supabase_connector\n\ndef create_datetime_features(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Create time series features from datetime index.\n\n    Only needed for none darts testing purposes.\n    Returns the dataframe with additional time series features.\n    \"\"\"\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    df['weekofyear'] = df['date'].dt.isocalendar()['week']\n    return df\n\n\ndef _transform_save_weather(df: pd.DataFrame, name: str) -&gt; pd.DataFrame:\n    \"\"\"Transform and save weather data.\n\n    Use one-hot encoding for weather conditions.\n    Save the transformed dataframe by given name to csv.\n    \"\"\"\n    df['date'] = df['datetime']\n    df = df.drop(columns=['datetime'])\n    df = df[['date', 'temp', 'tempmax', 'tempmin', 'conditions']]\n    df = pd.get_dummies(df, columns = ['conditions'], prefix='', prefix_sep='', dtype=int)\n    df.to_csv(f'../data/processed/{name}.csv', index=False)\n    return df\n\ndef _join_id_reason_to_covariates(covariates: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Join area_id and reason to the covariates Dataframe.\"\"\"\n    reason_df = pd.DataFrame()\n    absences_df = supabase_connector.load_dataframe(\"absences_daily\")\n\n    for reason in absences_df.reason.unique():\n        covariates.reason = reason\n        reason_df = pd.concat([covariates, reason_df])\n\n    reason_df_area = pd.DataFrame()\n\n    for area_id in absences_df.area_id.unique():\n        reason_df.area_id = area_id\n        reason_df_area = pd.concat([reason_df, reason_df_area])\n\n    reason_df_area.reset_index(inplace=True, drop=True)\n    reason_df_area['date'] = reason_df_area['date'].astype(str)   \n\n    return reason_df_area\n\ndef load_and_preprocess_weather() -&gt; pd.DataFrame:\n    \"\"\"Load Data and call above functions.\n\n    transforms and saves the weather data and returns the resulting Dataframe.\n    \"\"\"\n    weather_past_df = pd.read_csv('../data/external/weather_df.csv', delimiter=',', parse_dates=['datetime'])\n    weather_past_df = _transform_save_weather(weather_past_df, \"covariates_past_single\")\n    weather_past_df = _join_id_reason_to_covariates(weather_past_df)\n    weather_past_df.to_csv('../data/processed/covariates_past_multiple_m_reason.csv', index=False)\n    return weather_past_df\n\n</code></pre>"},{"location":"backend/data_prep/#update-covariates","title":"Update Covariates","text":"<p>This code updates the covariates table for the forecast of absences. Firstly, data is loaded from the 'absences_daily' and 'covariates' tables. A start date for the update is then defined, which is one day after the last date available in the covariates table. If new data is available, weather data for the period between the specified start date and today's date is retrieved and inserted into the covariates table.</p> update_covariates.py <pre><code>\"\"\"This script wraps the functions from crossingweather_connector and supabase_connector to update the covariates table.\"\"\"\nimport os\nimport pandas as pd\nimport json \nfrom datetime import date, timedelta\n\nimport data_scripts.supabase_connector as supabase_connector\nimport data_scripts.crossingweather_connector as crossingweather_connector\nfrom data_scripts.build_covariates import create_datetime_features\n\nabsences_df = supabase_connector.load_dataframe(\"absences_daily\")\ncovariates = supabase_connector.load_dataframe(\"covariates\")\n\nstartdate= str(covariates['date'].max().date() + timedelta(days=1))\nenddate = str(date.today())\n\ndef update_covariates() -&gt; None:\n    \"\"\"call functions from crossingweather_connector and supabase_connector to update the covariates table\"\"\"\n    if startdate &gt; enddate:\n        print('No new data available')\n    else:\n        covariates_df = crossingweather_connector.get_weather_data(covariates, startdate, enddate)\n        supabase_connector.insert_supabase('covariates', covariates_df)\n\n</code></pre>"},{"location":"backend/models/","title":"Models","text":"<p>In this section, the various models for prediction are defined, optimised, trained and recorded in more detail.</p>"},{"location":"backend/models/#logging-experiments","title":"Logging Experiments","text":"<p>This script implements functions for logging time series models in MLflow. The mlflow_ts function accepts a time series model, experiment names, time series, past covariates, run and model names, parameters, metrics and artefacts (images). It splits the time series into training and test sets, fits the model to the training data, creates an MLflow run and logs the model, parameters, metrics and artefacts using the _logging_mlflow function. The entire run is logged on a local MLflow server with the tracking URI 'http://localhost:5000'.  # mlflow logging function:</p> <pre><code>def mlflow_ts(model, experiment_name, timeseries, past_covariates, run_name, model_name, params:dict, metrics={}, artifacts={}, future_covariates=None)\n\n</code></pre> <p> The logging_saved.py script can be found in the /development/src/models directory. </p>"},{"location":"backend/models/#metrics","title":"Metrics","text":"<p>These functions calculate aggregated evaluation metrics for predictions of time series models in comparison to test time series. The metrics include the overall averaged RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), MAPE (Mean Absolute Percentage Error) and MASE (Mean Absolute Scaled Error). The calculations are carried out using the Darts library, whereby the functions are parallelised to several processor cores (by using the parameter n_jobs=-1).   We wanted to use the MAPE (Mean Absolute Percentage Error) as it provides a quick and easy-to-understand measure of how well a model performed. Furthermore, is a percentage metric of special interest because it makes it easier to compare the different models in which the absolute predicted absences may differ. However, calculating MAPE requires dividing by the actual absences, which are often zero for individual districts. As this division is not feasible, we chose to utilize alternative metrics such as RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), and MASE (Mean Absolute Scaled Error).   The metrics.py script can be found in the /development/src/models directory. </p>"},{"location":"backend/models/#models","title":"Models","text":"<p>This script contains functions for modelling and training various time series models, including LightGBM, XGBoost (XGB), TCN (Temporal Convolutional Network) and TFT (Temporal Fusion Transformer). It uses the Darts library for time series predictions and implements functions for logging models, parameters, metrics and artefacts (such as prediction plots) in MLflow. The models are trained with both standard parameters and optimised hyperparameters and applied to absence time series data. The train_models function calls all other functions to train the different models.   We decided to use for our models two gradient boosting models because of their efficiency in regard to the needed computation time and their general very good performances. The algorithm is grounded on a tree search algorithm that combines several weaker models to achieve better predictions. The implemented models are XGBoost and LightGBM which are the state-of-the-art algorithms. Usually, LightGBM achieves a higher accuracy while being faster than XGBoost, but on the downside is more susceptible to overfitting.</p> <p>In contrast to the gradient boosting models, we wanted to test two models which are based on the neural network technology. We chose the Temporal Convolutional Network (TCN) and the Temporal Fusion Transformer (TFT). TCN is specialized in extracting temporal patterns und relatively fast in comparison to other models based on neural networks architecture. The TFT architecture allows to capture intricate temporal relationships in data, by fusing information across multiple time series and related contextual data. Another advantage of the TFT architecture is that the feature importance can be measured which is not given for deep learning models. While TCN often requires less computing power than TFT, both TCN and TFT need more computational resources compared to gradient boosting models.   The models.py script can be found in the /development/src/models directory. </p>"},{"location":"backend/models/#optuna-hyperparameter","title":"Optuna Hyperparameter","text":"<p>This script contains functions for hyperparameter optimisation for various time series models. It uses the Optuna library to find the best hyperparameters for models such as TCN (Temporal Convolutional Network), XGBoost (XGB), LightGBM and TFT (Temporal Fusion Transformer). The script defines objective functions that train the models and minimise the MASE metric on a validation set. The best hyperparameters found are then returned. The script also uses the Darts library for time series predictions and the PyTorch Lightning library for training models with Lightning callbacks. The utilized hyperparameter for each model can be found in the Darts documentation.   The optuna_hyperparameter.py script can be found in the /development/src/models directory.</p>"},{"location":"backend/models/#timeseries","title":"Timeseries","text":"<p>The functions provided enable the loading and splitting of time series data for absence modelling. They transform data frames into Darts time series objects, split them for training and testing and enable the use of covariates for the prediction of absences over time.   The timeseries.py script can be found in the /development/src/models directory.</p>"},{"location":"backend/prediction/","title":"Prediction","text":""},{"location":"backend/prediction/#prediction-models","title":"Prediction Models","text":"<p>These functions use MLflow to load models for the prediction of past and future events. The models for past and future covariates are loaded separately. The predictions are then created using these models, transformed back and saved in CSV files for API calls.   The predictionmodels.py script can be found in the /development/src/prediction directory. </p>"},{"location":"backend/prediction/#prediction-timeseries","title":"Prediction Timeseries","text":"<p>The functions in this file update the data to the newest state. The functions _modify_absences, _make_future_covs, and _process_dfs process the DataFrames read in for absences, past and future covariates. Date modifications are made to ensure that the data is consistent. The function load_transform_to_timeseries_multiple then loads and transforms the modified DataFrames into Darts time series to prepare them for further processing in the models.   The predictiontimeseries.py script can be found in the /development/src/prediction directory. </p>"},{"location":"backend/prediction/#model-deployment","title":"Model deployment","text":"<p>The functions get_all_runs, get_prod_model_info and set_new_prod_model work together to identify the best model from the past and future experiments and move it to the production environment. set_production_pipeline calls these functions to move the models for both past and future experiments to the production stage. Older models are archived and the best model from each experiment is moved to the production stage if it is not already there.   The set_production.py script can be found in the /development/src/prediction directory. </p>"},{"location":"data/databaseModel/","title":"Database model","text":""},{"location":"data/overview_data/","title":"Internal data","text":"<p>Here you will find an overview of the data and its types provided by Mainpost to realise this project. </p> Abscences   Absences reflect person-related absences such as sickness and holidays.    Feature DataType Description number bigint id bigint employee_id bigint allocation_id bigint reason character varying Sick or on holiday start_date date end_date date temporary boolean Allocations Feature DataType Description id bigint employee_id bigint Number used for the link to the other tables (no link to the deliverer possible) round_id bigint Round key (letter, newspaper) round_code text Round code (link to rounds data) district_id bigint District code start_date text Start date of the allocation end_date text End date of the allocation level text For a round, the assignment with the highest level applies on the respective day (0 = regular deliverer, highest level = current deliverer) training boolean Labelling for instruction day_delivery boolean Indicator whether the round is held separately during the day and not at night -&gt; Only possible for letter rounds deposition_zip bigint Deposit location where the newspapers/letters are collected by the deliverer deposition_city text Deposit location where the newspapers/letters are collected by the deliverer deposition_district text Deposit location where the newspapers/letters are collected by the deliverer deposition_street text Deposit location where the newspapers/letters are collected by the deliverer Districts Feature DataType Description id bigint generated by default as identity name text District name for old districts number without great significance, for new district names postcode + number from 01 - XX (e.g.: 9707401) alias text Alias for district, usually empty round text Current round combination realm text Town and district (if available) active_since text Since when the district has been active split_allowed boolean Defines whether it is allowed to deliver the district in different delivery accesses area_id text Area number shelf text Wagon compartment number, where the letters are sorted or picked up is_old_district boolean Labelling whether it is an old district -&gt; District designation ZB____ avg_hours_old_districts double precision If it is an old district and no times exist for it, we use this value as the average time the district took to deliver on the day. avg_length_old_districts text If it is an old district and the distance has not yet been divided into the different laps and therefore this value can be used for the distance of the district vehicle_letter text Current transport vehicle vehicle_optimal_letter text Current or optimal transport vehicle vehicle_newspaper text Current transport vehicle vehicle_optimal_newspaper text Current or optimal transport vehicle newspapers text \u00d8 Number of newspapers letters bigint \u00d8 Number of letters direct text \u00d8 Number of direct distributions (magazines) direct_per_year text \u00d8 Days of direct distribution (magazines) per year meters_newspapers text Route for individual rounds within the district meters_letters text meters_dual text letter_district_surcharge text District-related surcharge if district is longer than actually planned and not yet remeasured newspaper_district_surcharge text District-related surcharge if district is longer than actually planned and not yet remeasured letter_employee_surcharge text Person-related surcharge (for current regular delivery staff) for handicap e.g. newspaper_employee_surcharge text Person-related surcharge (for current regular delivery staff) for handicap e.g. Employee Feature DataType Description id bigint Primary key to link the data with other tables (no reference to the person possible) weekly_hours text Only set for full-time/part-time employees type text 'default', 'auxiliary', 'time-based', 'variable' freelancer boolean monday boolean Whether the deliverer delivers on the respective day tuesday boolean wednesday boolean thursday boolean friday boolean saturday boolean sunday boolean birthday text expiration text Exit date expiration_reason text Exit reason nationality text date_of_joining text Date of entry residence_zip bigint residence_city text residence_district text residence_street text residence_latitude text residence_longitude text Rounds Feature DataType Description id bigint code text BMZ = Newspapers, BMB = Letters, for districts beginning with ZB 1 = Newspapers, 4 = Letter start_date text Start of the round end_date text End of the round monday boolean Whether the round was played on the day tuesday boolean wednesday boolean thursday boolean friday boolean saturday boolean sunday boolean Vacancies   Vacancies reflect the round-specific absences such as work prohibition, not worn, etc.    Feature DataType Description id bigint allocation_id bigint There is always an allocation for failure reason text Reason for failure: Not delivered, work ban, other start_date date end_date date"},{"location":"externalServices/servicesWebApp/","title":"Further services","text":""},{"location":"externalServices/servicesWebApp/#fullcalendar","title":"FullCalendar","text":"FullCalendar is a powerful JavaScript library used to create and display interactive calendars and event planners in web applications. This library allows to integrate appealing and user-friendly calendar views into their web applications. FullCalendar is particularly useful for applications that need to manage appointments, events and schedules.      <p> Check out our vue.js Calendar components</p>"},{"location":"externalServices/servicesWebApp/#mapbox","title":"Mapbox","text":"The delivery areas of the Mainpost Media Group are not only limited to the W\u00fcrzburg area. To get a better overview of the individual areas in our web application, we work with maps.                 Mapbox is a company that provides map services and geospatial tools. It enables developers to integrate custom maps and location services into their applications. Mapbox can be used to create interactive maps, geodata visualisation, location search, route planning and geographic analysis in a variety of applications and websites.                To be able to include Mapbox in our Vue.js frontend, the Mapbox-Gl-JS library was added.       Implementation of Mapbox in our application <pre><code>import mapboxgl from 'mapbox-gl';\nconst token = process.env.VUE_APP_MAPBOX_API_TOKEN\nmapboxgl.accessToken = token; \n// console.log('Token:', token);\n\nexport default mapboxgl;\n\n</code></pre> <p> Check out our vue.js Map component</p>"},{"location":"externalServices/servicesWebApp/#openweathermap","title":"OpenweatherMap","text":"As postal workers are usually facing the current weather conditions during most of their working day, we consider it an important component to include the current weather conditions in our models in order to make the prediction of downtimes even more accurate.                  OpenWeatherMap is a service that provides real-time weather data and information. It provides access to various weather data, including current weather conditions, forecasts, historical weather data and climate information.      <p>Following api queries to extract weather information from openweathermap have been executed:</p> Loading weather information for specific area <pre><code>## get weather information for next five days of specified area\nasync function getWeatherData(area_id)\n</code></pre> View our weatherService.js <pre><code>import axios from 'axios';\n\nexport async function getWeatherData(area_id) {\n    const apiKey = '768229ab8175e578773e450e2c81e5a3';\n    const days = 5;  // Number of days for the forecas\n    const location = getLocationCenter(area_id);\n\n    const forecastApiUrl = `https://api.openweathermap.org/data/2.5/forecast?q=${location}&amp;cnt=${days * 8}&amp;units=metric&amp;appid=${apiKey}`;\n    // console.log(\"try\");\n    let response;\n    try {\n        response = await axios.get(forecastApiUrl);\n        console.log(response);\n    } catch (error) {\n        console.error('Fehler beim Abrufen der Wetterdaten:', error);\n    }\n    return response;\n}\n\nfunction getLocationCenter(area_id) {\n    var location;\n    if (area_id == 1.1) {\n        location = \"Wuerzburg\";\n    } else if (area_id == 1.6) {\n        location = \"Wuerzburg\";\n    } else if (area_id == 1.7) {\n        location = \"Rimpar\";\n    } else if (area_id == 1.3) {\n        location = \"Gemuenden\";\n    } else if (area_id == 2.6) {\n        location = \"Gerolzhofen\";\n    } else if (area_id == 2.5) {\n        location = \"Hammelburg\";\n    } else if (area_id == 2.3) {\n        location = \"Werneck\";\n    } else if (area_id == 1.2) {\n        location = \"Helmstadt\";\n    } else if (area_id == \"FN\") {\n        location = \"Tauberbischofsheim\";\n    } else if (area_id == 1.4) {\n        location = \"Kitzingen\";\n    } else if (area_id == 1.5) {\n        location = \"Ochsenfurt\";\n    } else if (area_id == 2.7) {\n        location = \"Oerlenbach\";\n    } else if (area_id == 2.8) {\n        location = \"Stadtlauringen\";\n    }else if (area_id == 2.4) {\n        location = \"Oberelsbach\";\n    }else {\n        location = \"Wuerzburg\";\n    }\n    return location;\n}\n\n</code></pre>"},{"location":"frontend/absences/","title":"Absence Registration","text":"<p>This component provides a user-friendly interface for recording information on future absences or substitutions.  The component comprises several steps. Firstly, the employee IDs are retrieved when the page is loaded and displayed dynamically in a drop-down menu. The drop-down list is searchable, which makes it easier to select the employee ID.   The actual form contains various input fields such as date, reason, name, district ID, round ID, training and residence ZIP. Error messages for required fields are displayed when filling in the form.   The form is submitted using the createFutureVacancy method from the replacementService module. The form is checked for validity before submission. Error messages are displayed if required fields have not been filled in correctly. Once the form has been successfully submitted, a success message is displayed.  </p> <p> Check out our vue.js VacancyDetection component</p> <p>The following api queries were executed to extract information about possible replacements of absent employees:</p> API queries possible replacements <pre><code>## find absent employee replacement according to last runners of the specific round\nasync function findReplacement(round_id)\n\n## perform api call to fastapi-backend to retrieve optimal replacement\nasync function findOptimalReplacement(date, reason)\n\n## create operation to database to insert new row to future vacancy after filling out form\nasync function createFutureVacancy(data) \n\n</code></pre>"},{"location":"frontend/dashboard/","title":"Dashboard","text":"<p>The dashboard provides the user with a comprehensive overview of various information and functions. It includes profile information, company details and the status of the user account. To get an initial overview of current figures, \"Lieferausf\u00e4lle je Jahr\" and \"Verteilung der Verkehrsmittel f\u00fcr Briefe\" are loaded on the dashboard in the shape of bar or line charts. Users can choose between the chart types to visualise the data in different ways.   The dashboard also contains a map with data on the \"W\u00fcrzburg\" location. The points shown provide a graphical overview of the areas in which employees are absent. A calendar is available right next to it, which contains absences listed for the selected month. The calendar can be edited by the user by manually adding absences for individual employees.</p> <p></p> <p> Check out our vue.js DashboardComponent</p>"},{"location":"frontend/districts/","title":"Districts","text":""},{"location":"frontend/districts/#district-list","title":"District List","text":"<p>This page provides the user with a table containing information on various districts. Each record in the table represents a district with corresponding details. The \"Ansehen\" button takes the user to a detailed view of a specific district. The table is paginated, which means that only a limited number of data records are displayed at once. Below the table is a navigation bar with page numbers and navigation arrows to scroll through the pages and view more districts. The user can switch between pages to view different records.</p> <p></p> <p> Check out our vue.js DistrictList component</p>"},{"location":"frontend/districts/#district-details","title":"District Details","text":"<p>The page displays detailed information about a specific district, including name, alias, number of laps, geographic area, activation date and more. This information is clearly organised in info boxes. There is also a map component that visually displays the geographic location of the district. This allows the user to get both textual and visual insights into the selected district.  </p> <p></p> <p> Check out our vue.js DistrictDetail component</p> <p>The following database queries were executed to extract district information:</p> Database query district information <pre><code>## get all districts between index &gt; startNumber &amp; index &lt; endNumber\nasync function getAllDistricts(startNumber, endNumber)\n\n## get specific district information for detailview\nasync function getSpecificDistrict(districtId) \n\n## get num of districts with max 2 filters\nasync function getDistrictCount(filter = null, filter_two = null) \n\n## get zip codes of one specific District\nasync function getZipsOfDistrict(districtId)\n\n## get count of different vehicle types with which letters get carried out\nasync function getVehicleLetterCounts()\n\n## get all 16 area codes \nexport async function getAllAreas()\n\n</code></pre>"},{"location":"frontend/employees/","title":"Employees","text":""},{"location":"frontend/employees/#employee-list","title":"Employee List","text":"<p>The page provides an overview of employee statistics, including the number of active employees, the proportion of full-time employees and other relevant key figures.   The main view shows two bar charts that visualise the distribution of employees and the yearly development. There is also a table of employees that can be filtered by status, employment type and other criteria. Each employee is represented by a line with ID, city, status and employment type. The table contains a pagination function for easy navigation through the data records.</p> <p></p> <p> Check out our vue.js EmployeeList component</p>"},{"location":"frontend/employees/#employee-details","title":"Employee Details","text":"<p>This page displays detailed information on a specific employee.   If the employee is active, further statistics on sick days in 2021, 2022 and 2023 are displayed. A list of employee information, including birthday, employment type, place of residence and street, is also available.  In addition, there is a table that lists the employee's absences (holiday or illness). The table is displayed either with the relevant data or an empty list, depending on whether the employee has absences.</p> <p></p> <p> Check out our vue.js EmployeeDetail component</p> <p>The following database queries were executed to extract employee information:</p> Database query employee information <pre><code>## get all employees between index &gt; startNumber &amp; index &lt; endNumber\nasync function fetchEmployees(startNumber, endNumber) \n\n## get all active Employees of Year xx\nasync function getActiveEmployeesInYear(year)\n\n## get all employees according to max 2 filters\nasync function fetchFilteredEmployees(startNumber, endNumber, filterOne = null, filterTwo = null)\n\n## get num of employees according to filters\nasync function getEmployeeCount(filter = null, filter_two = null) \n\n## get all vacant employees from date to another date\nasync function getFutureVacancies(from, to)\n\n## get employee ids\nasync function fetchEmployeeIds() \n\n</code></pre>"},{"location":"frontend/historical/","title":"Historical","text":"<p>In this section of our application, we take a look at the historical development of employee absences, delivery failures and rounds data. Interactive diagrams illustrate patterns and developments in these areas. The following sections provide insights into historical trends to gain valuable knowledge from past operational developments.</p>"},{"location":"frontend/historical/#absences","title":"Absences","text":"<p>This component displays a chart of monthly employee absences based on the selected year and reason (holiday or sickness). Users can select the year and reason via drop-down lists. The Chart.js library is used to create an interactive bar chart that visualises the monthly absences for the selected year and reason.</p> <p> </p> <p> Check out our vue.js MonthlyAbsences component</p>"},{"location":"frontend/historical/#delivery-failures","title":"Delivery Failures","text":"<p>This component shows a chart of monthly failures related to failed deliveries for a selected year and a selected reason (holiday, illness or not delivered). Users can select the year and reason via drop-down lists. The chart is also created with Chart.js and visualises the monthly downtime for the selected year and reason.   Check out our vue.js YearlyVacancies component</p>"},{"location":"frontend/historical/#rounds","title":"Rounds","text":"<p>This Vue.js component creates a page with statistics that contains two charts. The statistics show delivery failures per year and delivery failures per round code.   Check out our vue.js RoundStatistics component</p>"},{"location":"frontend/landingpage/","title":"Landing Page","text":"<p>The static HTML landing page offers users a concise overview of the application. The functions, the technology stack used and access to the login area are clearly presented here. After logging out of the application, users are automatically redirected to this page.  </p>"},{"location":"frontend/login/","title":"Login","text":"<p>After all container are started, you can login to the application on Netlify. Login with you credentials. Your username is the first letter of the name and the full surename.  </p> <p></p> <p>The login process is performed using the loginWithSupabase method, which uses the Supabase authentication services and redirects to the /dashboard page if the login is successful.</p> <p> Check out our vue.js LoginComponent component</p> <p>The following queries were executed to extract authentification information:</p> Authentification functions <pre><code>## retrieve auth State from session when changes occur\nasync function getAuthState()\n\n## get all users \nasync function fetchUsers()\n\n## get currently logged in user from session\nasync function fetchLoggedUser()\n\n</code></pre>"},{"location":"frontend/planning/","title":"Planning","text":"<p>Our platform provides a comprehensive picture of various aspects, starting with Weather-Related Cancellations, where advanced forecasting models and real-time weather data support advanced proactive measures to manage outages. General Outages provides strategic insights into long-term outage forecasts, while Substitute Workers enables real-time tracking of employee absences and efficient replacement planning. These approaches are designed to optimise processes and improve the organisation's adaptability.  To conclude, the Model Performance section provides a holistic view of our platform by enabling users to thoroughly test the effectiveness of the implemented machine learning models.</p>"},{"location":"frontend/planning/#weather-related-cancellations","title":"Weather-Related Cancellations","text":"<p>In this predictive view, an advanced model with multiple time series forecasts outages for the next five days. Real-time weather data from OpenWeatherMap is seamlessly integrated, providing valuable insight into weather conditions and their impact on outages.</p> <p>The user-friendly interface can be customised by selecting one of 16 different areas via a drop-down menu. This feature allows users to focus on specific geographic regions, making it easier to implement targeted proactive measures in response to expected outage patterns.</p> <p>The user interface provides a dual forecast: a comprehensive weather forecast for the next five days and a visual representation of expected outages based on correlated weather conditions. This forecast is presented through an intuitive line graph that helps users recognise patterns and make informed decisions.</p> <p>By integrating state-of-the-art forecasting models with real-time weather data, our platform not only improves the accuracy of outage prediction, but also empowers users with the knowledge to proactively address potential issues. This integrated approach encourages a proactive response to outage scenarios and contributes to a more resilient and better prepared infrastructure.   Check out our vue.js VacancyPrediction component</p>"},{"location":"frontend/planning/#general-outages","title":"General Outages","text":"<p>In this advanced forecast view, users are provided with the ability to forecast outages up to three years into the future, providing strategic insights for long-term planning.   It is important to emphasise that weather-related considerations are excluded due to the inherent unpredictability over longer periods of time. The view includes filter options at the top, allowing users to refine forecasts based on specific reasons such as sickness or holidays, as well as date ranges. This functionality proves invaluable for gaining a comprehensive overview of future periods and making proactive adjustments to employee scheduling to respond to anticipated patterns of absence.   Check out our vue.js AbsencePredictionFiltered component</p>"},{"location":"frontend/planning/#substitute-workers","title":"Substitute Workers","text":"<p>This comprehensive view allows users to track the absence of individual employees on specific days and understand the reasons for each absence. The information is dynamically updated as soon as absences are recorded by the previously introduced components. The view makes it possible to switch between a weekly and daily perspective, providing a differentiated understanding of absence patterns.</p> <p>Additionally, the view provides access to information on designated replacements, which streamlines the process of ensuring coverage during employee absences. This feature-rich view enhances workforce management by providing real-time insight into employee availability and enabling proactive planning for smooth operations.</p> <p></p> <p>When the user selects one of the buttons above, two different approaches are available for selecting a replacement. The first approach utilises a machine learning model to intelligently determine the most suitable substitutes based on various factors. This advanced model takes into account the employee's skills, past performance and other relevant parameters to optimise the selection process.</p> <p>Alternatively, users can decide in favour of the traditional company approach, where one of the previous employees traces the path. This utilises the experience of previous employees to find a reliable and efficient replacement and ensure continuity of service delivery.</p> <p></p> Check out our vue.js EmployeeAllocation component <pre><code>&lt;template&gt;\n    &lt;ListCalendar /&gt;\n\n&lt;/template&gt;\n\n&lt;script&gt;\n    import  ListCalendar  from \"@/components/SubComponents/Calendar/ListCalendar\";\n\n    export default {\n        data() {\n            return {\n                replacement: [],\n            }\n        },\n        mounted() {\n            this.loadData();\n        },\n        methods: {\n            async loadData() {\n            }\n        },\n        components: {\n            ListCalendar,\n        },\n    }\n\n\n&lt;/script&gt;\n\n</code></pre>"},{"location":"frontend/planning/#model-performance","title":"Model Performance","text":"<p>In this final view of our web application, users can explore and analyse the performance of our machine learning models through visual representations. By using MLflow to track models, performance metrics are presented using embedded images that provide detailed insights into key parameters. MLflow's specialised features, such as model tracking, make the view an informative and visually engaging experience. Users can seamlessly evaluate and compare the efficiency of different machine learning models. These models are predictions for weather-related employee shortages. </p>"},{"location":"frontend/planning/#feature-influence-of-the-weather-data","title":"Feature Influence of the weather data","text":"<p>Machine Learning models work often as black boxes making it challenging to understand the results and estimate their validity. To assess our outcomes were different approaches employed. One such method we utilized was the explanation technique called SHAP (Shapley Additive Explanations). SHAP Values enable the calculation of feature importance for each data observation within a Machine Learning model. Subsequently, a bee swarm plot illustrating the feature importance of weather data is presented. The total feature importance decreases from top to bottom. Consequently, we found that minimal temperature stands out as the weather variable with the highest impact. Higher values for this variable positively contribute to predictions, while lower values have a negative contribution.  </p>"},{"location":"frontend/planning/#effect-of-the-parameters-on-performance","title":"Effect of the parameters on performance","text":""},{"location":"frontend/planning/#probabilistic-prediction","title":"Probabilistic prediction","text":"<p>The model performance is compared using Area_ID and the reason for failure as a probabilistic prediction. This means that we do not limit the predictions of the models to whether a failure will occur or not, but also analyse the probabilities for different reasons in different areas.    This probabilistic approach makes it possible to gain deeper insights into the predictive accuracy of the models both at the level of working areas (Area_ID) and on the basis of different reasons for failure. Through this analysis, patterns and trends can be identified that help to better understand the strengths and weaknesses of the models in different contexts and to make targeted optimisations.  This forecast displays the 90% quantile of predictions, indicating the likelihood of predictions falling within the correct range as seen in the example below.</p> <p></p>"},{"location":"projectSetup/dockerSetup/","title":"Docker Setup","text":""},{"location":"projectSetup/dockerSetup/#vuejs-frontend","title":"Vuejs - Frontend","text":"<p>Right now in docker compose the dev dockerfile gets executed.</p>"},{"location":"projectSetup/dockerSetup/#dockerfile-for-dev","title":"Dockerfile for dev","text":"<ul> <li>Builds the container for the frontend application. </li> </ul> Frontend Dockerfile <pre><code># Stage 1: Build Vue.js app\nFROM node:lts-alpine as build-stage\n\n# Set the working directory to the /frontend folder inside the container\nWORKDIR /app/frontend\n\n# Install Vue CLI globally\nRUN npm install -g @vue/cli\n\n# Copy the package.json and package-lock.json files to the container\nCOPY ./frontend/package*.json ./\n\n# Install project dependencies, including the newly added package(s)\nRUN npm install \n\nRUN npm install --save \\\n  @fullcalendar/core \\\n  @fullcalendar/vue3\n\n# Copy /frontend from the host to the /frontend folder inside the container\nCOPY ./frontend .\nCOPY ./frontend/.env .env\n\n# Run the app\nCMD [\"npm\", \"run\", \"serve\"]\n\nEXPOSE 8080\n\n</code></pre>"},{"location":"projectSetup/dockerSetup/#dockerfile-for-production","title":"Dockerfile for production","text":"<ul> <li>Builds the container for the frontend application for later to be realised production scenario.</li> </ul> Production Dockerfile <pre><code># Stage 2: Serve the built app using Nginx\nFROM nginx:stable-alpine as production-stage\n\n\n# Copy the Nginx configuration file into the image\nCOPY my-nginx-config.conf /etc/nginx/conf.d/default.conf\n\n# Copy the built app from the build-stage to Nginx web server's root folder\nCOPY --from=build-stage /app/dist /usr/share/nginx/html\n\n# Expose port 80 for Nginx\nEXPOSE 80\n\n# Start Nginx and keep it running in the foreground\nCMD [\"nginx\", \"-g\", \"daemon off;\"]\n\n</code></pre>"},{"location":"projectSetup/dockerSetup/#fast-api-backend","title":"Fast - API Backend","text":"<ul> <li>Builds the container for the backend/api implementation. </li> </ul> Backend Dockerfile <pre><code># Use the Python base image\nFROM python:3.9\n\n# Set work directory in the container\nWORKDIR /app\n\n# Create a virtual environment\nRUN python -m venv /venv\n\n# Activate the virtual environment\nENV PATH=\"/venv/bin:$PATH\"\n\n# Copy the application files to the work directory in the container\nCOPY ./app/app.py /app/app.py\n\n# Installation of the required python dependencies\nCOPY ./docker/backend/requirements.txt /app/requirements.txt\n\n# Installation of the required python dependencies\nRUN pip install -r /app/requirements.txt\n\n# Installing Rust\nRUN apt-get update &amp;&amp; apt-get install -y rustc\n\n# Start the FastAPI application with Uvicorn and activate the live reload\nCMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n\n</code></pre>"},{"location":"projectSetup/dockerSetup/#mlflow-modeltracking","title":"MLFlow - Modeltracking","text":"<ul> <li>Builds the container for the mlflow instance which is used for the model tracking. </li> </ul> MLFlow Dockerfile <pre><code># Use the Python base image\nFROM python:3.9\n\n# Set work directory in the container\nWORKDIR /mlflow\n\n# Installation of the required Python dependencies for MLflow\nRUN pip install mlflow psycopg2-binary sqlalchemy\n\n# Start the MLflow server when starting the container\nCMD [\"mlflow\", \"server\", \"--backend-store-uri\", \"/mlflow/mlruns\", \"--default-artifact-root\", \"/mlflow/mlartifacts\", \"--host\", \"0.0.0.0\", \"--port\", \"5001\"]\n\n</code></pre>"},{"location":"projectSetup/dockerSetup/#cronjob-implementation","title":"Cronjob Implementation","text":"<ul> <li>Builds the container for the mlflow instance which is used for the model tracking. </li> </ul> Cronjob Dockerfile <pre><code># Verwenden Sie das Python-Basisimage\nFROM python:3.9\n\n# Arbeitsverzeichnis im Container festlegen\nWORKDIR /code\n\n# Kopieren Sie die Anwendungsdateien in das Arbeitsverzeichnis im Container\nCOPY ./development/src/update_data.py /code/update_data.py\nCOPY ./development/src/update_predictions.py /code/update_predictions.py\nCOPY ./development/src/retrain_models.py /code/retrain_models.py\n\n# Installieren der erforderlichen python Abh\u00e4ngigkeiten\nCOPY docker/cron/requirements.txt /code/requirements.txt\n\n# Installieren der erforderlichen Python-Abh\u00e4ngigkeiten\nRUN pip install -r /code/requirements.txt\n\n# Updating packages and installing cron\nRUN apt-get update &amp;&amp; apt-get -y install cron\n\n# Adding crontab to the appropriate location\nCOPY docker/cron/crontab /etc/cron.d/crontab\n\n# Giving permission to crontab file\nRUN chmod 0644 /etc/cron.d/crontab\n\n# Running crontab\nRUN /usr/bin/crontab /etc/cron.d/crontab\n\n# Creating entry point for cron \n\nCMD [\"cron\",\"-f\", \"-l\", \"2\"]\n\n</code></pre> <p> Our Dockerfiles can be found in the /docker directory. </p>"},{"location":"projectSetup/installation/","title":"Installation","text":"<p>Make sure you have docker desktop installed on your local machine</p> <ul> <li>Minimum of 6GB Memory allocated in Docker Desktop and 25 GB of free Diskspace (Open Docker Desktop -&gt; Settings -&gt; Resources)</li> </ul> <p>You need a GitHub account</p>"},{"location":"projectSetup/installation/#project-setup","title":"Project Setup","text":""},{"location":"projectSetup/installation/#github-repositories","title":"Github Repositories","text":"<p>Clone our Github Repository:</p> HTTPS <pre><code>git clone https://github.com/UHPDome/backend_mainpost.git\n\n</code></pre> SSH <pre><code>git clone git@github.com:UHPDome/backend_mainpost.git\n\n</code></pre>"},{"location":"projectSetup/installation/#edit-envexample","title":"Edit .env.example","text":"<pre><code>VUE_APP_SUPABASE_KEY=value\nVUE_APP_MAPBOX_API_TOKEN=value\n</code></pre> <p>The corresponding file can also be found in /frontend</p>"},{"location":"projectSetup/installation/#project-directory","title":"Project directory","text":"<p>Navigate into the project directory:</p> <pre><code>cd backend_mainpost\n</code></pre>"},{"location":"projectSetup/installation/#start-docker-container","title":"Start Docker Container","text":"<p>Start the Docker containers:</p> <pre><code>docker-compose up\n</code></pre> <p>Note:</p> <ul> <li>initial build can take up to 30 minutes</li> <li>we ran into problems while building in the eduroam network because of firewall and proxy policies</li> <li>If you are on a Windows Machine, use VSCode</li> <li>If you want to enable cron scheduling you have to change the crontab from CLRF to LF. You can do this by clicking on the icon on the bottom right corner:</li> </ul> <p></p>"},{"location":"projectSetup/installation/#docker-composeyml","title":"docker-compose.yml","text":"Edit setup in docker-compose.yml file <pre><code>version: \"3.9\"\nservices:\n  update_predictions:\n    container_name: 'dockerized-update-predictions'\n    build:\n      context: .\n      dockerfile: ./docker/development/Dockerfile.execute  \n    volumes:\n      - ./development/src:/code\n    depends_on: \n      - mlflow\n    environment:\n      - MLFLOW_TRACKING_URI=http://host.docker.internal:5001\n    #extra_hosts:\n    #  - \"host.docker.internal:host-gateway\"\n  fastapi:\n    build:\n      context: .\n      dockerfile: ./docker/backend/Dockerfile\n    ports:\n      - \"8000:8000\"\n    volumes:\n#      - ./app:/app\n      - .:/app\n  mlflow:\n    build:\n      context: .\n      dockerfile: ./docker/mlflow/Dockerfile  # Verweisen Sie auf das oben gezeigte Dockerfile\n    ports:\n      - \"5001:5001\"\n    volumes:\n      - ./development/src/mlruns:/mlflow/mlruns\n      - ./development/src/mlartifacts:/mlflow/mlartifacts\n  frontend:\n    container_name: 'dockerized-mainpost-frontend'\n    build: \n      context: ./\n      dockerfile: './docker/frontend/Dockerfile'\n    stdin_open: true\n    tty: true\n    ports:\n      - \"8080:8080\"\n    volumes:\n      - ./frontend:/app/frontend\n      - /app/frontend/node_modules\n    environment:\n      - CHOKIDAR_USEPOLLING=true\n  #cronjob:\n  #  container_name: 'dockerized-cronjob'\n  #  build:\n  #    context: .\n  #    dockerfile: ./docker/development/Dockerfile  # Verweisen Sie auf das oben gezeigte Dockerfile\n  #  ports:\n  #    - \"7000:7001\"\n  #  volumes:\n  #    - /etc/timezone:/etc/timezone:ro # sync time zone with host machine\n  #    - /etc/localtime:/etc/localtime:ro\n  #    - ./development/src:/code\n\n</code></pre>"}]}